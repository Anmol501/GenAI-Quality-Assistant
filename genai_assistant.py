# -*- coding: utf-8 -*-
"""GenAI_Assistant.ipynb

Automatically generated by Colab.

"""

from google.colab import files

uploaded_files = files.upload()

import os

documents = []

for filename in uploaded_files.keys():
    with open(filename, 'r', encoding='utf-8', errors='ignore') as file:
        text = file.read()
        documents.append({
            "filename": filename,
            "content": text
        })

print(f"Loaded {len(documents)} documents")

!pip install -q langchain faiss-cpu sentence-transformers

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

all_chunks = []

for doc in documents:
    splits = text_splitter.split_text(doc["content"])
    for i, chunk in enumerate(splits):
        all_chunks.append({
            "text": chunk,
            "source": doc["filename"],
            "chunk_id": i
        })

print(f"Total chunks created: {len(all_chunks)}")

pip install -U langchain-community

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

chunk_texts = [c["text"] for c in all_chunks]

vectorstore = FAISS.from_texts(texts=chunk_texts, embedding=embedding_model)

print("FAISS index created!")

!pip install -q openai langchain openrouter

import os

os.environ["OPENROUTER_API_KEY"] = "Use your own key lol"

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
    model="mistralai/mistral-7b-instruct",
    base_url="https://openrouter.ai/api/v1",
    api_key=os.environ["OPENROUTER_API_KEY"],
    temperature=0.2
)

from langchain.chains import RetrievalQA

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

print("RAG QA Chain ready!")

query = "Can you help me make a FMEA model for an imaginary situation like me being late in class? You can take your necessary assumptions."
response = qa_chain(query)

print("Answer:")
print(response["result"])

file_path = "/content/AssessmentMSME.txt"

with open(file_path, "r", encoding="utf-8") as f:
    raw_text = f.read()

print("File loaded. Sample text:\n")
print(raw_text[:1000])

from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

chunks = splitter.split_text(raw_text)

print(f"Total chunks created: {len(chunks)}")
print("Sample chunk:\n", chunks[0])

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

vectordb = FAISS.from_texts(chunks, embedding_model)

print("‚úÖ FAISS vector index created!")

from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

pipe = pipeline("text2text-generation", model="google/flan-t5-base", max_length=512)

llm = HuggingFacePipeline(pipeline=pipe)

retriever = vectordb.as_retriever(search_kwargs={"k": 3})

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

query = "We have lots of defects. What quality tools can we use?"
response = qa_chain.invoke({"query": query})
print("Answer:\n", response["result"])

query = "How can an MSME improve customer delivery timelines?"

response = qa_chain.invoke({"query": query})

print("Answer:\n", response["result"])

print("\n Sources:")
for doc in response["source_documents"]:
    print(doc.page_content[:200], "...\n")

import re

with open("/content/AssessmentMSME.txt", "r", encoding="utf-8") as f:
    data = f.read()

pattern = r"\n(\d+\. .+?)(?=\n\d+\. |\Z)"
matches = re.findall(pattern, data, flags=re.DOTALL)

assessment_items = {}
for i, block in enumerate(matches):
    lines = block.strip().split("\n")
    question = lines[0].strip()
    suggestion = "\n".join(lines[1:]).strip()
    assessment_items[question] = suggestion

print(f"Parsed {len(assessment_items)} assessment items.")

from collections import defaultdict
import re

def parse_assessment_file(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()

    lines = content.splitlines()

    structure = defaultdict(dict)
    current_section = None
    current_subtopic = None

    for line in lines:
        line = line.strip()
        if not line:
            continue
        if re.match(r"^[A-Z]\.\s", line):
            current_section = line
            current_subtopic = None
        elif re.match(r"^\d+\.", line):
            current_subtopic = line
            structure[current_section][current_subtopic] = []
        elif re.match(r"^[-‚Ä¢‚óè]", line):
            if current_section and current_subtopic:
                structure[current_section][current_subtopic].append(line)
    return structure

structure = parse_assessment_file("/content/AssessmentMSME.txt")
print("Assessment structure loaded!")

def start_assessment_with_tracking(structure):
    print("Welcome to MSME Self-Assessment\n")
    print("We'll go section by section. Reply with:")
    print("  y = yes, assess this section\n  n = skip this section\n  p = pause assessment\n")

    responses = {}

    for section, subtopics in structure.items():
        user_input = input(f"\n Section: {section}\nDo you want to assess this section? (y/n/p): ").lower().strip()

        if user_input == "n":
            responses[section] = {"skipped": True, "answers": {}}
            continue
        elif user_input == "p":
            print("Assessment paused. You can resume later.")
            break
        elif user_input == "y":
            section_answers = {}
            for subtopic, bullets in subtopics.items():
                print(f"\nüîπ {subtopic}")
                for point in bullets:
                    print(f"   {point}")
                answer = input("Do you follow this practice? (y/n): ").lower().strip()
                section_answers[subtopic] = answer
            responses[section] = {"skipped": False, "answers": section_answers}
        else:
            print("Invalid input. Skipping section.")
            responses[section] = {"skipped": True, "answers": {}}

    return responses

user_responses = start_assessment_with_tracking(structure)

def generate_summary_report(responses):
    summary = []
    summary.append("MSME Quality Self-Assessment Summary\n")
    summary.append("="*50)

    for section, data in responses.items():
        if data["skipped"]:
            summary.append(f"\n Skipped Section: {section}")
        else:
            summary.append(f"\n Section: {section}")
            for subtopic, answer in data["answers"].items():
                if answer == "y":
                    summary.append(f"  {subtopic} ‚Äî Followed")
                elif answer == "n":
                    summary.append(f"  {subtopic} ‚Äî Needs Improvement")
                else:
                    summary.append(f"  {subtopic} ‚Äî No response")

    return "\n".join(summary)

report_text = generate_summary_report(user_responses)

print(report_text[:2000])

with open("MSME_Quality_Assessment_Report.txt", "w", encoding="utf-8") as f:
    f.write(report_text)

print("Report saved as MSME_Quality_Assessment_Report.txt")

from collections import defaultdict
import re

with open("/content/AssessmentMSME.txt", "r", encoding="utf-8") as f:
    assessment_text = f.read()

lines = [line.strip() for line in assessment_text.splitlines() if line.strip()]

recommendation_dict = {}
current_subtopic = None
current_points = []

for line in lines:
    if re.match(r"^\d+\.\s", line):
        if current_subtopic:
            recommendation_dict[current_subtopic] = "üìå " + " ".join(current_points)
        current_subtopic = re.sub(r"^\d+\.\s*", "", line)
        current_points = []
    else:
        current_points.append(line)

if current_subtopic and current_points:
    recommendation_dict[current_subtopic] = "üìå " + " ".join(current_points)

print(f"Extracted {len(recommendation_dict)} recommendations.")

import random
for topic in random.sample(list(recommendation_dict.keys()), 5):
    print(f"{topic}\n{recommendation_dict[topic]}\n")

!pip install reportlab

from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
from datetime import datetime

def generate_pdf_report(user_answers, recommendations_dict, filename="MSME_Quality_Report.pdf"):
    doc = SimpleDocTemplate(filename, pagesize=A4)
    styles = getSampleStyleSheet()
    story = []

    story.append(Paragraph("MSME Quality Assessment Report", styles['Title']))
    story.append(Spacer(1, 12))
    story.append(Paragraph(f"Date: {datetime.now().strftime('%d %B %Y')}", styles['Normal']))
    story.append(Spacer(1, 24))

    for topic, result in user_answers.items():
        status = "Good" if result == "y" else "Needs Improvement"
        story.append(Paragraph(f"<b>{topic}</b> ‚Äî {status}", styles['Heading3']))

        if result != "y":
            suggestion = recommendations_dict.get(topic, "No specific recommendation found.")
            story.append(Paragraph(suggestion, styles['Normal']))

        story.append(Spacer(1, 12))

    doc.build(story)
    print(f"Report generated: {filename}")

user_answers = {}

for section, data in user_responses.items():
    if not data["skipped"]:
        for subtopic, answer in data["answers"].items():
            user_answers[subtopic] = answer

generate_pdf_report(user_answers, recommendation_dict, filename="MSME_Quality_Assessment_Report.pdf")

from google.colab import files
files.download("MSME_Quality_Assessment_Report.pdf")

from graphviz import Digraph
from IPython.display import Image

flow_input = "Start -> Define problem -> Measure delivery times -> Analyze delays -> Improve scheduling -> Control via dashboards -> End"

steps = [step.strip() for step in flow_input.split("->")]

dot = Digraph(format='png')
for i in range(len(steps) - 1):
    dot.edge(steps[i], steps[i + 1])

dot.render("pdca_flowchart", cleanup=False)
Image("pdca_flowchart.png")

from graphviz import Digraph
from IPython.display import Image

def detect_shape(step):
    step_lower = step.lower()
    if 'start' in step_lower or 'end' in step_lower:
        return 'ellipse'
    elif any(word in step_lower for word in ['if', 'decision', 'decide', 'whether']):
        return 'diamond'
    elif any(word in step_lower for word in ['input', 'output', 'collect', 'receive']):
        return 'parallelogram'
    else:
        return 'rectangle'

def generate_flowchart(flow_input, filename="flowchart"):
    steps = [step.strip() for step in flow_input.split("->")]

    dot = Digraph(format='png')
    for step in steps:
        shape = detect_shape(step)
        dot.node(step, step, shape=shape)

    for i in range(len(steps) - 1):
        dot.edge(steps[i], steps[i + 1])

    dot.render(filename, cleanup=False)
    return Image(filename + ".png")

flow_input = "Start -> Collect data from customers -> Decide improvements -> Implement changes -> End"

generate_flowchart(flow_input)

!pip install -q langchain openai tiktoken chromadb faiss-cpu
!pip install -q sentence-transformers

with open("/content/AssessmentMSME.txt", "r", encoding="utf-8") as f:
    msm_text = f.read()

print(msm_text[:1000])  # Preview first 1000 chars

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# 1. Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.create_documents([msm_text])

# 2. Embed using sentence-transformers
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db = FAISS.from_documents(chunks, embedding_model)

print(f"Loaded {len(chunks)} chunks into vector DB.")

import os
os.environ["OPENAI_API_KEY"] = "Use your own key lol"

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(openai_api_base="https://openrouter.ai/api/v1",
                 openai_api_key=os.environ["OPENAI_API_KEY"],
                 model_name="gpt-3.5-turbo",
                 temperature=0.3)

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    chain_type="stuff"
)

query = "Tools to measure customer satisfaction?"
answer = qa_chain.run(query)
print(answer)

import os
from langchain.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

all_docs = []

for filename in uploaded_files:
    filepath = os.path.join("/content", filename)
    if filename.endswith(".txt"):
        loader = TextLoader(filepath)
    elif filename.endswith(".pdf"):
        loader = PyPDFLoader(filepath)
    else:
        print(f"Skipping unsupported file: {filename}")
        continue
    all_docs.extend(loader.load())

print(f"Loaded {len(all_docs)} chunks from your files.")

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
docs = text_splitter.split_documents(all_docs)
print(f"Total chunks after splitting: {len(docs)}")

embeddings = HuggingFaceEmbeddings()
vectordb = FAISS.from_documents(docs, embeddings)
print("FAISS vector store created.")

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

conversational_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
    memory=memory,
    return_source_documents=True,
    output_key="answer",
    verbose=True
)

while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        print("Goodbye!")
        break
    try:
        result = conversational_chain.invoke({"question": user_input})
        print("\n Bot:", result["answer"])
    except Exception as e:
        print("Error:", e)

